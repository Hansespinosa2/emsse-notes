\section{Ljung Systems Identification}
\subsection{Introduction}
\subsubsection{Dynamic Systems}
A system is some kind of object that interact with and produce signals.
An input $u$ enters the system that is controllable by the user alongside a set of uncontrollable but observable inputs $w$.
There are additional disturbances that influence the system but are not observable or controllable $v$.

\subsubsection{Models}
A model is some kind of an assumed relationship between observed signals of a system.
There can be mental models, graphical models, software models, and analytical models.
We build a model from observed data, regardless of the type of model.

To build a mathematical model, we can 1) split up the system into subsystems that have already been "created" and combine these subsystems into a final model of the whole system or 2) we can use experimentation to try analyze experimental data of input and output to create a model.
The first type, that of subsystems, is called \textit{modeling} and the second type, with experimentation, is called \textit{system identification}.

Generally, we care about the usefulness of a model as we compare it to the system.

\subsubsection{Example Problem with ARX}
A system's inputs and outputs can be represented with $u(t), y(t)$ at each time step $t$.
We can use the linear difference equation to show a basic relationship between the two.
\begin{equation}
    y(t) + a_1 y(t-1) + \dots + a_n y(t-n) = b_1 u(t-1) + \dots + b_m u(t-m)
\end{equation}
This can be rearranged 
\begin{equation}
    y(t) = - a_1 y(t-1) - \dots - a_n y(t-n) + b_1 u(t-1) + \dots + b_m u(t-m)
\end{equation}
and then rewritten as
\begin{equation}
    \hat{y}(t) = \phi^\top (t) \theta
\end{equation}
where
\begin{align}
    \theta = 
  \begin{bmatrix}
    a_1 \\ \vdots \\ a_n \\ b_1 \\ \vdots \\ b_m
  \end{bmatrix}
  & \quad \phi =  
  \begin{bmatrix}
    -y_(t-1) \\ \vdots \\ -y(t-n) \\ u(t-1) \\ \vdots \\ u(t-m)
  \end{bmatrix}
\end{align}

In the event that we have a dataset $Z^N$, a collection of $N$ inputs and outputs, we can use the least squares method to select the value that minimizes the least squares of the distance between the parameter selected values and the real output

\begin{equation}
    \hat{\theta} = \arg \min_\theta \frac{1}{N} \sum_{i=1}^{N} (y(t) - \hat{y}(t | \theta))^2
\end{equation}

where we can substitute the prediction $\hat{y}(t|\theta)$ with $\phi^\top(t) \theta$.
This can be solved by setting the derivative to zero and the least squares problem has the closed form solution.

The vector $\phi(t)$ is called the regression vector, and the components are the regressors.
Models that contain old values of the output $y(t-1), y(t-2), etc$ are called auto regressors, leading to the model class ARX (Auto-Regression with eXtra inputs).

\subsubsection{Systems Identification Procedure}
In order to construct a model we need a dataset, a set of candidate models, and a rule by which to assess the model.
Generally, a model structure is a parametrized mapping from past inputs and outputs to the space of the model outputs, that has been trained on the dataset.
\begin{equation}
    \hat{y}(t|\theta) = g(\theta, Z^{l-t})
\end{equation}
A black box model is a model where we have no physical consideration of the system, and a model with some physical interpretation is a gray box.
Experiment design can be used to create a maximally informative dataset, we can use background information and modeling to identify a set of candidate models, and we can use a few testing techniques to use the gathered data to approximate the model's performance on the overall system.
We can then identify if the model is good enough and perform model validation, at which we then go back through the loop if it is not.

\begin{figure}[htbp]
  \centerline{\includegraphics[width=0.50\textwidth]{../../images/sys_des_loop.png}}
  \caption{System Design Loop}
  \label{fig:sys_des_loop}
\end{figure}

In order to select a good model, the following information is helpful:
\begin{enumerate}
    \item Identification techniques and rational, and typical choices of model steps
    \item Knowledge of the properties of the identified model and their dependence on data, the model set, and identification criterion.
    \item Numerical schemes for computing the estimate
    \item Sufficient knowledge to make intelligent choices of experiment design, model set, and identification criterion.
\end{enumerate}

\subsection{Time-Invariant Linear Systems}
\subsubsection{Impulse Responses, Disturbances, and Transfer Function}
A system is considered \textit{time invariant} if its response to a certain input signal does not depend on absolute time, it is \textit{linear} if the output response to a linear combination of inputs is the same linear combination of the output responses of the individual inputs, and it is \textit{causal} if the output at a time $t$ depends only on $0,\dots,t$, that is the time up to $t$.
A linear, time-invariant, causal system can be described by its impulse response $g(\tau)$
\begin{equation}
    y(t) = \int_{\tau=0}^{\infty} g(\tau) u(t-\tau) d \tau
\end{equation}

for discrete time,
\begin{equation}
    y(t) = \sum_{k=1}^{\infty} g(k) u(t-k)
\end{equation}

This is an oversimplification the majority of the time as the output is not perfectly characterized by the inputs.
Instead, we have disturbances that add some random signals that we can not observe into the system.
We can represent these disturbances as $v(t)$, which can represent the sum of measurement noise, uncontrollable or unobservable inputs, or other random sources.
\begin{equation}
    y(t) = \sum_{k=1}^{\infty} g(k) u(t-k) + v(t)
\end{equation}
Most disturbances are only noticeable by their effect on the output.
The value of a disturbance is not known beforehand.
The disturbance can be defined as a convolution with an impulse response $h(k)$ of the disturbance and a white noise variable $e(t)$
\begin{equation}
    v(t) = \sum_{k=0}^{\infty} h(k) e(t-k)
\end{equation}
These white noise (zero mean, constant variance, no correlation between points OR a sequence of independent random variables with zero mean and variances $\lambda$) distributions can come in many forms, such as a constant (positive and negative) value applied with probability $\mu$ every $t$ time step, or a normal distribution with mean $0$. 

The second-order properties of $v$ refer to the mean $E[v(t)] = 0$ and the variance $E[v(t) v(t-\tau)] = \lambda \sum_{k=0}^{\infty} h(k) h(k-\tau)$.

This book uses $q, q^{-1}$ for the forward and backward shift operator respectively.
\begin{equation}
    q^{-1} u(t) = u(t-1)
\end{equation}
and we can therefore rearrange the impulse response function convolution equation into a transfer function form:
\begin{gather}
    y(t) = \sum_{k=1}^{\infty} g(k) u(t-k) = \sum_{k=1}^{\infty} g(k) (q^{-k} u(t)) \\ 
    = [\sum_{k=1}^{\infty} g(k) q^{-k}] u(t) = G(q) u(t)
\end{gather}
where $G(q) = \sum_{k=1}^{\infty} g(k) q^{-k}$.

which can be extended to the case of the disturbances
\begin{equation}
    v(t) = H(q) e(t)
\end{equation}
and to the overall case of a linear system with additive disturbances
\begin{equation}
    y(t) = G(q) u(t) + H(q) v(t)
\end{equation}

The transfer function $G(z)$ is stable if the sum of the impulse responses until infinity converge.
\begin{equation}
    \sum_{k=1}^{\infty} |g(z)| < \infty
\end{equation}
which coincides with the definition of BIBO (bounded input, bounded output) stability.
This also means that the function $G(z)$ is analytic on and outside the unit circle and has no poles in that area.
For a rational transfer function $G(q)$, the zeros are the zeros of the numerator and the poles are the zeros of the denominator.

A filter is monic if its zeroth coefficient is 1
\begin{equation}
    H(q) = 1+ \sum_{k=1}^{\infty} h(k) q^{-k}
\end{equation}

\subsubsection{Frequency-Domain Expressions - Skipped}
\subsubsection{Signal Spectra - Skipped}
\subsubsection{Single Realization Behavior and Ergodicity}
Provided the stochastic part of a signal can be described as filtered white noise, and hence brought into $v$, then the spectrum of an observed single realization of $s(t)$, computed for a deterministic signal, coincides, with probability 1, with that of the process $s(t)$, defined by ensemble averages.
Basically, the theory of probability is useful even when you only have limited data.

\subsubsection{Multivariable Systems}
We can change the assumed single-input single-output models into a multivariable model with $p$ output components and $m$ input components.
These systems are multivariable where $\mathbf{y} \in \mathbb{R}^p, \mathbf{x} \in \mathbb{R}^m$.
The easy part of this change is notational changes and ensuring that transposes are maintained and that the general structure and logic of the problem is adhered to.
The difficult part arises when there $p > 1$, and there are multiple outputs.

\begin{equation}
    \mathbf{y}(t) = \textbf{G} (q) \textbf{u}(t) + \textbf{H}(q) e(t)
\end{equation}

where $\mathbf{y} \in \mathbb{R}^p, \mathbf{x} \in \mathbb{R}^m, \textbf{G} \in \mathbb{R}^{p \times m}, \textbf{H} \in \mathbb{R}^{p \times p}, \textbf{e} \in \mathbb{R}^p$.
This is the basic linear system with disturbances with multiple inputs and multiple outputs.
The $i,j$-th entry of $\textbf{G}$ is the scalar transfer function from input number $j$ to output number $i$.
